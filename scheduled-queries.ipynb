{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setting up scheduled queries for Big Query on GCP"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "\n",
    "Why would we want to schedule a query, etc.\n",
    "\n",
    "While creating a scheduled query is quite easy, there are a few things you need to consider when making and deploying a scheduled query for production. This article will go over different ways of setting up scheduled queries and how well each is suited for production ready systems.\n",
    "\n",
    "## What is needed of a production ready scheduled query deployment\n",
    "\n",
    "1. **The schedule of the query.**\n",
    "This one might seem obvious, but if we can't define at what time the query is to run, we can't schedule a query\n",
    "\n",
    "2. **First run of the query.**\n",
    "If you think this is taken care of in the schedule, you are might risk getting some odd reports. If you want weekly averages calculated every Friday, keyed to the weeknumber. But your first query run happens on a Wednesday, not only will you have an average that spans a different set of days, come Friday, you will have 2 aggregates for the same week, but with different numbers.\n",
    "\n",
    "3. **Query and deployment as code, in seperate files.**\n",
    "Your devOps script that can deploy your query, so that the same thing can be deployed across testing, development and production environments and your developer wants to be able to edit the query in its own `.sql` file, so that their development tools can assist them, and so they don't have to touch deployment code when they are developing the query.\n",
    "\n",
    "4. **Backfill, historical runs.**\n",
    "You need to be able to re-run queries for past dates. If something went wrong in production and the scheduled query didn't have access to all numbers on Friday, you want to be able to re-run it on Monday *as if it was Friday*. You might also want to be able to create a report for Friday last year - long before the query was scheduled.\n",
    "\n",
    "5. **Control the schema of the destination table.**\n",
    "If the results produced by the scheduled query are large, and expected to run for a long time, the destination table should be "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The data\n",
    "\n",
    "For this tutorial, we will imagine we are in the business of lightning strikes and management has decided that weekly aggregates of lightning strikes are critical to growth going forward.\n",
    "\n",
    "We are also going to imagine that aggregating this data is an operation that is so heavy, that it doesn't make sense to do it on the fly.\n",
    "\n",
    "The [National Oceanic and Atmospheric Administration (NOAA)](https://www.noaa.gov) publishes times and locations of all observed lightning strikes, and this is available as a public dataset in BigQuery. The dataset is licensed through [Vaisala](https://www.vaisala.com/en/products/data-subscriptions-and-reports/data-sets/nldn) and according to Google is [freely available to the public, under data.gov license](http://www.data.gov/privacy-policy#data_policy). The data can be accessed in BigQuery as `bigquery-public-data:noaa_lightning`-dataset.\n",
    "\n",
    "To play with it yourself, or any of the other many public datasets available in BigQuery, open the console, click `+ ADD DATA`>`Pin Project` and pin \"*bigquery-public-data*\".\n",
    "\n",
    "## The Query of interest\n",
    "\n",
    "As a start, we use the following query, but we will expand it, as it becomes clear that this is too simple for a production environment\n",
    "\n",
    "```SQL\n",
    "SELECT\n",
    "  SUM(number_of_strikes) as n_strikes\n",
    "FROM\n",
    "  `bigquery-public-data.noaa_lightning.lightning_*`\n",
    "WHERE\n",
    "  day BETWEEN DATE_SUB(CURRENT_DATE(), INTERVAL 1 week)\n",
    "  AND CURRENT_DATE()\n",
    "```\n",
    "\n",
    "This query will return the sum of lightning strikes that happened during each lightningstrike observation between the date that the query was run, and 1 week before the query was run. If we run this weekly, we will build up a table of weekly averages. If we run it daily, we will build up a table of rolling, weekly averages.\n",
    "\n",
    "*A note about the asterisk* The lightning table is partitioned into several tables. By querying from `lightning_*` we tell Big Query to look at all the lightning tables, instead of just the table with 2019 data (if we wrote `lightning_2019`). You can uptimize the query here, since we normally won't need to look at 2017 data to do a job on 2019 data. But just around new year, we might have queries that do span 2 tables, so be careful how you optimize here. In this article, I have gone with the most straightforward solution.\n",
    "\n",
    "## The Schedule\n",
    "\n",
    "We want to run this every Sunday morning, 00:00Z and aggregate the past week\n",
    "\n",
    "## The Raw data\n",
    "The raw data looks like the following, and can be plotted on a map using BigQuery Geoviz\n",
    "\n",
    "![](assets/lighning_strike_raw_viz.png)\n",
    "\n",
    "## The Destination table\n",
    "\n",
    "The final result will be a table that displays the weekly number of lightning strikes. Each additional row is generated by the end of the week and added to the table. For this data, such a simple query can be made on the fly, but if we imagine that the results are much more complex and may span hundreds of of row for each week, you can hopefully imagine why you'd want to do it on a schedule, rather than on the fly.\n",
    "\n",
    "![](assets/destination_table_preview.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Using the Bigquery Console\n",
    "\n",
    "This is by far the most straight-forward way of working with, and trying out features and services in GCP, and it is a nice place to start, as you get a great IDE for writing and testing your query.\n",
    "\n",
    "You simply write your query and and click create scheduled query and choose when and how often you want it to run.\n",
    "\n",
    "![Screenshot of user about to create Schedule query in GCP Console](assets/create_scheduled_query.png)\n",
    "\n",
    "You can run the query before you set it up, and you can easily get an overview of most of the settings available to you. You should definitely start every prototype here.\n",
    "\n",
    "![Screenshot of user filling out the settings of a scheduled query in the GCP Console](assets/scheduled_query_options.png)\n",
    "\n",
    "Clicking \"Schedule\" will run the query, create a new table, and append the result to that table. Easy as that.\n",
    "\n",
    "However, neither the query, nor the setup of the query are defined as code and version controlled. This is not good for reproduceability, further development or any kind if production environment.\n",
    "\n",
    "There are some other things that aren't quite right with this query yet. When we get to week two, we will have a hard time identifying what time period a given result belongs to. Moreover, we are only able to calculate aggregates into the future. What if we want aggregates from dates *before* we deployed the scheduled query?\n",
    "\n",
    "## Adding Backfill\n",
    "\n",
    "Backfill, or historical queries, is to run a scheduled query with a date set in the past. This is usefull if a query fail and you need to re-run it, or if you need the query to be run *last* week, instead of just next week.\n",
    "\n",
    "If we instead of using `CURRENT_DATE()`-function to select for the last 7 days, we use the `@run_date` parameter, we can ask BigQuery to do a manual run of the scheduled query with `@run_date` substituted for any date we like, so our SQL will now look like this:\n",
    "\n",
    "```SQL\n",
    "SELECT\n",
    "  SUM(number_of_strikes),\n",
    "  @run_date as last_week_from\n",
    "FROM\n",
    "  `bigquery-public-data.noaa_lightning.lightning_*`\n",
    "WHERE\n",
    "  day BETWEEN DATE_SUB(@run_date, INTERVAL 1 week)\n",
    "  AND @run_date\n",
    "```\n",
    "\n",
    "I also added the date as a column, so results from different runs can be identified.\n",
    "\n",
    "Note that once the query is parameterised, you can no longer execute it from the console. Checks to see if the query is still valid will also often fail, making it difficult to update the query."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Using the Cloud SDK\n",
    "\n",
    "We also want to store our SQL code in VCS and deploy the query as part of our CD/CI-pipeline. For this, it is not feasible to have a developer log on to the webconsole and manually do deployments. So we need to move away from the webconsole, in order to test the parameterised query as well as having deployment script that DevOps can automate.\n",
    "\n",
    "One way is to use the Cloud SDK from the shell, e.g. the `gcloud` and the `bq` shell commands. Often these are the easiest ways to interact with the GCP.\n",
    "\n",
    "According to the documentations there are 2 different commands to deploy scheduled queries, and it is not immediately clear which is preferred:\n",
    "\n",
    "1. `bq query` followed by a bunch of flags and finally the query\n",
    "2. `bq mk --transfer_config`, hooks more directly into the [BigQuery Transfer service](https://cloud.google.com/bigquery-transfer/docs/), which Scheduled Queries are a part of. Uses a mix of commandline flags and JSON."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using `bq query`\n",
    "\n",
    "![](assets/bq_query_header.png)\n",
    "\n",
    "Most of the flags are fairly self explanatory, and our deployment command will roughly be the following, but needs a few more refinements.\n",
    "\n",
    "```Bash\n",
    "bq query \\\n",
    "    --use_legacy_sql=false \\\n",
    "    --destination_table=lightning.test \\\n",
    "    --display_name='Weekly Lightning' \\\n",
    "    --append_table \\\n",
    "    --schedule='every sun 00:00' \\\n",
    "'SELECT\n",
    "  SUM(number_of_strikes),\n",
    "  @run_date as last_week_from\n",
    "FROM\n",
    "  `bigquery-public-data.noaa_lightning.lightning_*`\n",
    "WHERE\n",
    "  day BETWEEN DATE_SUB(@run_date, INTERVAL 1 week)\n",
    "  AND @run_date'\n",
    "```\n",
    "\n",
    "### Remember the schedule flag\n",
    "\n",
    "Note that if you forget the `--schedule`-flag (as [Google did in their example code](https://cloud.google.com/bigquery/docs/scheduling-queries#setting_up_a_scheduled_query), at the time of writing) you will be met with a very cryptic error:\n",
    "\n",
    "```\n",
    "Error in query string: Error processing job 'scheduled-queries:bqjob_r3159aed97aa830f5_0000016dd9faa75c_1':\n",
    "Undeclared parameter 'run_date' is used assuming different types (DATE vs INT64) at [3:3]\n",
    "```\n",
    "\n",
    "This is because without the `--schedule` flag, the `@run_time` SQL-parameter will not have a value, so BigQuery will fail at type-checking your query. It first assumes the empty parameter must be a INT64 in line 3, and then later sees that it is used as a date.\n",
    "\n",
    "This is the same error that causes you to be unable to save edits this query in the webconsole.\n",
    "\n",
    "### The interval format of the schedule flag\n",
    "\n",
    "Another under-documented feature here is what values are accepted by the `--schedule` flag. It follows the [App-engine flexible environment's cron.yaml-syntax](https://cloud.google.com/appengine/docs/flexible/python/scheduling-jobs-with-cron-yaml#the_schedule_format). The easiest way to create these is to use the console to create the desired  scheduled with point and click, and then  inspect the final result in the completed schedule under the configuration tab.\n",
    "\n",
    "### Don't use false for either append or replace flags\n",
    "\n",
    "Both the `--append_table` and `--replace` are shown in the documentation as taking a `true` value. This is strictly not necessary, you don't need to write `--replace=true` if you want to truncate the table everytime the job is run, and if you don't want the table truncated, you actually cannot write `--replace=False`.\n",
    "\n",
    "- Add the `--replace` flag for replacing tables (What BQ Transfer internally calls WRITE_TRUNCATE as your write_disposition value)\n",
    "- Add the `--append_table` flag if you want to append to the destination table (What BQ Transfer internally calls WRITE_APPEND as your write_disposition value)\n",
    "- If you add both flags or none of them, you will append.\n",
    "\n",
    "## Working with an external .sql file\n",
    "\n",
    "But of course we want the deployment command seperated from the .sql file. Let us see what we can do in BASH:\n",
    "\n",
    "```bash\n",
    "# deploy_query.sh\n",
    "\n",
    "bq query \\               \n",
    "    --use_legacy_sql=false \\ \n",
    "    --destination_table=lightning.test \\\n",
    "    --display_name='Weekly Lightning' \\            \n",
    "    --append_table \\\n",
    "    --schedule='every sun 00:00' \\                \n",
    "    `cat weekly.sql`\n",
    "```\n",
    "\n",
    "## Backfill\n",
    "\n",
    "To run the query on historical dates looks fairly simple\n",
    "\n",
    "```bash\n",
    "bq mk \\\n",
    "    --transfer_run \\\n",
    "    --start_time='start_time' \\\n",
    "    --end_time='end_time' \\\n",
    "    [resource_name]\n",
    "```\n",
    "\n",
    "However, we don't know the name of the newly created query ressource! \n",
    "\n",
    "The ressource name is returned by the `bq query` command on success and looks something like\n",
    "\n",
    "```bash\n",
    "Transfer configuration 'projects/655462874078/locations/us/transferConfigs/5dafef56-0000-2a29-a894-f4f5e80c659c' successfully created.\n",
    "```\n",
    "\n",
    "and we can cut it out with a bit of more bash\n",
    "\n",
    "```bash\n",
    "# deploy_query.sh\n",
    "\n",
    "bq query \\               \n",
    "    --use_legacy_sql=false \\ \n",
    "    --destination_table=lightning.test \\\n",
    "    --display_name='Weekly Lightning' \\            \n",
    "    --append_table \\\n",
    "    --schedule='every sun 00:00' \\                \n",
    "    `cat weekly.sql`| tee /dev/tty | cut -d \"'\" -f 2 > resource_name.txt\n",
    "```\n",
    "\n",
    "Here we pipe the result of the query deployment to [tee](https://en.wikipedia.org/wiki/Tee_(command)), where we can save it to our logs (or in this example, just print it to the terminal), and then pipe the original result to the [cut](https://en.wikipedia.org/wiki/Cut_(Unix)) utility, which will cut it into fields on the single quotation mark, and we save the second field to the filesystem.\n",
    "\n",
    "```bash\n",
    "# backfill.sh\n",
    "\n",
    "start_time=@1\n",
    "end_time=@2\n",
    "\n",
    "bq mk \\\n",
    "    --transfer_run \\\n",
    "    --start_time=$start_time \\\n",
    "    --end_time=$end_time \\\n",
    "    `cat resource_name.txt`\n",
    "```\n",
    "\n",
    "## Reviewing `bq query` way of deployment\n",
    "\n",
    "Using Google's Cloud SDK we can create a deployment with seperate SQL and deployment files that can be stored in version control and deploy on different environments. We can run backfill on historical dates.\n",
    "\n",
    "We can not control the schema of the destination table. One workaround is to create the table in a seperate command in `deploy_query.sh`, but this requires that the developer to write the schema by hand. Not impossible, but combersome and requires a lot of testing.\n",
    "\n",
    "We also cannot control when the first query is run. With `bq query ...` command, the query will run immediately and save rows to the destination table. A workaround could be to follow the deployment with a query that deletes all rows after the first run, so that over time, only data from the scheduled events (and backfills) will exist in the destination table. Also not impossible, but not particularly elegant."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using `bq mk --transfer_config`\n",
    "![](assets/bq_mk_transfer_config_header.png)\n",
    "\n",
    "This command mimics more directly the transfer service API. Most flags corrosponds to a field in the [TransferConfig Resource](https://cloud.google.com/bigquery-transfer/docs/reference/datatransfer/rest/v1/projects.locations.transferConfig), although this isn't completely one-to-one.\n",
    "\n",
    "Moreover, this command uses a mix of commandline flags and in-line JSON. From the perspective of someone who wants to deploy a scheduled query, it is not obvious what kind of information goes into the JSON and what should be a commandline flag. But roughly speaking the JSON describes the table, and the flags describes the schedule. In this context, the query also describes the layout of the table, as the result of a query is a table. An overview of all available parameters are not documented anywhere.\n",
    "\n",
    "Let us start with a simplified command, and try and expand it.\n",
    "\n",
    "```bash\n",
    "bq mk --transfer_config \\\n",
    "    --target_dataset=lightning \\\n",
    "    --display_name='Weekly Lightning' \\\n",
    "    --params='{\"query\":\"SELECT 1 from mydataset.test\",\"destination_table_name_template\":\"test\",\"write_disposition\":\"WRITE_APPEND\", \"partitioning_field\": \"last_week_from\"}' \\\n",
    "    --schedule=schedule-string \\\n",
    "    --schedule_start_time=datetime \\\n",
    "    --data_source=scheduled_query\n",
    "```\n",
    "\n",
    "The documentation for all the flags and parameters used are scattered between the `bq mk --help` command, the [scheduled query documentation page](https://cloud.google.com/bigquery/docs/scheduling-queries) and the [TransferConfig Resource documentaion page](https://cloud.google.com/bigquery-transfer/docs/reference/datatransfer/rest/v1/projects.locations.transferConfig).\n",
    "\n",
    "\n",
    "*Note* \n",
    "- `--data_source` flag *must* be \"scheduled_query\", or else we are not creating a scheduled query.\n",
    "- `--schedule_start_time`: \"Time to start scheduling transfer runs for the given transfer configuration. If empty, the default value for the start time will be to run immediately. The format for the time stamp is [RFC3339 UTC \"Zulu\".](https://developers.google.com/protocol-buffers/docs/reference/google.protobuf#google.protobuf.Timestamp)\" - manpage for `bq mk`\n",
    "\n",
    "### Start time\n",
    "\n",
    "If we don't supply a start time, the query will run as soon as we deploy it. If we deploy on Thursday, we will suddenly have a summary for the wrong timespan, since we expect weekly summaries Sunday-Sunday.\n",
    "\n",
    "We can change when the first run of the schedule is, by using the `--schedule_start_time`-flag. However, unlike its name or man-page description, this flag doesn't specify *when* the first run is, it specifies a time which the schedule must run no sooner than. Simply supplying the current time will get you what you want. Adding the current time is fine, just remember to convert it to UTC.\n",
    "\n",
    "If you are on a BSD system (such as MacOS) or a GNU system (such as Linux) the `date` program will behave very differently, but for displaying the current time in UTC \"Zulu\", it works out just the same.\n",
    "\n",
    "```bash\n",
    "date -u +%Y-%m-%dT%TZ\n",
    "```\n",
    "\n",
    "### Partitioning the destination table\n",
    "\n",
    "It is generally a good idea to partition your tables. This makes it possible to drastically reduce the size of your queries later on. This is not possible to do, when using the `bq query`-command. Which column to partition on is defined in the `partitioning_field`-field inside the JSON parameters.\n",
    "\n",
    "### Splitting out the query and writing the deployment files\n",
    "\n",
    "In terms of splitting this command into a SQL-file, a deployment script and a backfill script, the major hurdly is to insert the SQL query string into the JSON string and the JSON string into the `bq mk` command.  (we'll tackle `--schedule_start_time` a little later)\n",
    "\n",
    "If we naively `cat` the sql-query directly into the json string,\n",
    "\n",
    "```bash\n",
    "...\n",
    "    --params='{\"query\":\"'`cat weekly.sql`'\",\"destination_table_name_template\":\"test2\",\"write_disposition\":\"WRITE_APPEND\"}' \\\n",
    "...\n",
    "```\n",
    "\n",
    "We will get a FATAL error\n",
    "\n",
    "```\n",
    "FATAL Flags positioning error: Flag '--schedule=every' appears after final command line argument. Please reposition the flag.\n",
    "Run 'bq help' to get help.\n",
    "```\n",
    "\n",
    "This is because of the line breaks that `cat` prints from the file. In bash, if you echo a variable with quotes, it will retain new lines, but if you don't quote the variable, it will have newlines removed. Using this, we can split the loading of the query and the json construction into seperate operation and get:\n",
    "\n",
    "```bash\n",
    "# deploy_query-transfer_config.sh\n",
    "\n",
    "query=`cat weekly.sql`\n",
    "json='{\n",
    "\"destination_table_name_template\" : \"test\",\n",
    "\"write_disposition\": \"WRITE_APPEND\",\n",
    "\"partitioning_field\": \"last_week_from\",\n",
    "\"query\": \"'${echo $query}'\"\n",
    "}'\n",
    "\n",
    "bq mk --transfer_config \\\n",
    "    --target_dataset=lightning \\\n",
    "    --display_name='Weekly Lightning' \\\n",
    "    --schedule='every sunday 00:00' \\\n",
    "    --schedule_start_time=\"`date -u +%Y-%m-%dT%TZ`\" \\\n",
    "    --data_source=scheduled_query \\\n",
    "    --params=\"$json\" | tee /dev/tty | cut -d \"'\" -f 2 > resource_name.txt\n",
    "```\n",
    "\n",
    "This will also work with quotes inside of the SQL query. Here we also store the resource name, just like we did with the `bq query` command, and backfill can be done just like earlier.\n",
    "\n",
    "### Backfill\n",
    "\n",
    "This is done just as we did previously."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Links\n",
    "\n",
    "- [Scheduled Queries documentation](https://cloud.google.com/bigquery/docs/scheduling-queries)\n",
    "- [BigQuery Transfer documentation](https://cloud.google.com/bigquery-transfer/docs/working-with-transfers)\n",
    "- [BigQuery Transfer's client library documentation](https://cloud.google.com/bigquery-transfer/docs/reference/libraries#client-libraries-usage-python)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
